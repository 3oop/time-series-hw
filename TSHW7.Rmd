---
title: "Time Series Chapter 7 HW"
author: "Pooria Assarehha"
date: "2024-01-06"
output: pdf_document
---

## Ex 7.1

$n = 100$   $r_1 = 0.8$     $r_2 = 0.5$     $r_3 = 0.4$     $\bar{Y} = 2$

$s^2 = 5$     $Y_t \sim$ AR(2) + $\mu$

$\hat{\phi}_1, \hat{\phi}_2, \hat{\theta}_0, \hat{\sigma}^2_e = ?$

Using Yule-Walker Estimations:

$$
\begin{cases}
r_1 = \phi_1 + r_2\phi_2 \\
r_2 = r_1\phi_1 + \phi_2 
\end{cases} \quad \Rightarrow \quad \hat{\phi}_1 = \frac{r_1(1-r_2)}{1-r_2^2}  = 1.\bar{1}  \quad , \quad \hat{\phi}_2 = \frac{r_2-r_1^2}{1-r_1^2} \approx -0.39 
$$

```{r}
phi1h = 0.8*(1-0.5)/(1-0.8^2)
phi2h = (0.5 - 0.8^2)/(1-0.8^2)
phi1h
phi2h
```


$$\begin{cases}
Y_t - \mu = \phi_1(Y_{t-1} - \mu) + \phi_2(Y_{t-2} - \mu) + e_t \\
Y_t = \phi_1Y_{t-1} + \phi_2Y_{t-2} + e_t  + \theta_0
\end{cases} \quad \Rightarrow \quad \theta_0 = -\phi_1\mu + \phi_2\mu + \mu $$

$$\theta_0 = \mu(1-\phi_1 - \phi_2) \Rightarrow \hat{\theta}_0 = \hat{\mu}(1-\hat{\phi}_1 - \hat{\phi}_2) \quad \Rightarrow \hat{\theta}_0 = 2(1 - 1.11 - (-0.389) ) = 0.\bar{55}$$
```{r}
theta0h = 2*(1 - phi1h - phi2h )
theta0h
```

$$\hat{\sigma}_e^2 = \left(1 - \hat{\phi}_1r_1 - r_2\hat{\phi}_2\right)s^2 = 1.527$$

```{r}
sigma2h = 5* (1- phi1h*0.8 - phi2h*0.5)
sigma2h
```

## Ex 7.3
-- $Y_t \sim$ AR(1)
-- $\phi \approx 0.7$
-- $Z_{0.975} \hat{Var}(\phi) \leq  0.1 \Rightarrow  n=?$

$$
AR(1) \Rightarrow \hat{\phi} = r \Rightarrow Var(r_1) = \frac{1-\phi_1^2}{n} \quad \Rightarrow 1.96\times \sqrt{\frac{1-0.7^2}{n}} \leq 0.1  \Rightarrow n \geq (10\times1.96)^2 \times 0.51 \Rightarrow \quad n \geq 196
$$

```{r}
 0.51 * (10*1.96)^2
```

## Ex 7.4

-- $Y_t \sim$ MA(1)
-- $\mu = 0$
-- $n=3$
-- $Y_1 = 0$
-- $Y_2 = -1$
-- $Y_3 = \frac{1}{2}$
-- (a) Cond-LS $\hat{\theta} = \frac{1}{2}$
-- (b) $\hat{\sigma}^2_e = ?$

### 7.6.a

$$e_t = Y_t + \theta e_{t-1} \begin{cases} e_1 = Y_1 + 0 = 0 \\ e_2 = Y_2 + \theta e_1 = -1 \\ e_3 = Y_3 + \theta e_2 = \frac{1}{2} + \theta(-1)\end{cases} \Rightarrow S_c(\theta) = \sum_{t=1}^3 e_t^2 = 0 + 1 + (\frac{1}{2} - \theta)^2$$

$$\frac{\partial S_c}{\partial \theta} = 2\theta - 1 = 0 \Rightarrow \hat{\theta} = \frac{1}{2}$$
### 7.6.b

$$\hat{\sigma}^2_e = \frac{\sum_{t=1}^n e_t^2}{n-1} = \frac{1}{3-1} = \frac{1}{2}$$

Note: Since the mean is known to be zero, one might reasonably argue that the divisor should be n rather than nâˆ’1.

## Ex 7.6

Model I: $Y_t - \mu = \phi (Y_{t-1}-\mu) + e_t$
Model II: $Y_t = \phi Y_{t-1}+ \theta_0 + e_t$

$$S_c(\phi, \mu) = \sum_{t=2}^n (Y_t - \mu - \phi(Y_{t-1} - \mu))^2 = \sum_{t=2}^n (Y_t - \mu - \phi Y_{t-1} - \phi\mu)^2$$

$\phi\mu$ causes non-linearity

$$S_c(\phi, \theta_0) = \sum_{t=2}^n (Y_t - \phi Y_{t-1} + \theta_0)^2 = \sum_{t=2}^n \left( (Y_t - \phi Y_{t-1})^2 + 2\theta(Y_t - \phi Y_{t-1}) + \theta^2_0 \right) \quad \Rightarrow \quad \begin{cases}
\frac{\partial S_c}{\partial \phi} = 2\phi \sum_{t=2}^n Y_{t-1} - 2\sum_{t=2}^n Y_t Y_{t-1} - 2\theta_0 \sum_{t=2}^n Y_{t-1} = 0\\
\frac{\partial S_c}{\partial \theta_0} = 2(n-1)\theta_0 + 2\sum_{t=2}^n (Y_t - \phi Y_{t-1}) = 0
\end{cases}$$

## Ex 7.10

for a simulated MA(1) with $\theta = -0.6$

    -- Method of Moments: $\hat{\theta} = ?$
    -- Con LS of the prior and compare? 
    -- MLE of the prior and compare?

we have to negate $\theta$ due to difference in MA definition 

```{r}
library(TSA)
yt = arima.sim(list(ma=c(0.6)), n=36)
plot(yt)
```



```{r}
r = acf(yt, ci.type='ma', plot = FALSE)
rho = ARMAacf(ma = c(0.6))
```

For MA(1) we have the Moments Estimate:

$$\hat{\theta} = \frac{-1 \pm \sqrt{1- 4r_1^2}}{2r_1}$$
```{r}
r1 = r[[1]][1]
(-1 + c(1,-1)*sqrt(1-4*r1^2))/(2*r1)
```

for invertibility we choose $\hat{\theta} = -0.31$, which is very far from reality. 

we know MM Estimates are not good for MA series, The Cond LS Estimate cannot be obtained by hand.

```{r}
arima(yt, order = c(0,0,1), method = "CSS")
```

gives us $\hat{\theta} = 0.5833$ which is much more acceptable.


```{r}
arima(yt, order = c(0,0,1), method = "ML")
```

gives us $\hat{\theta} = 0.5660$ which is much more acceptable.

